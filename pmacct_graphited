#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Copyright (C) 2014 Bernd Zeimetz <bernd@bzed.de>
Copyright (C) 2014 conova communications GmbH

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

from setproctitle import setproctitle
from application.process import process, ProcessError

from watchdog.observers import Observer
from watchdog.events import RegexMatchingEventHandler, FileSystemEvent
from setproctitle import setproctitle

import time
import os
import re
import sys
import signal
import json

import graphitesend

import logging
import logging.handlers

class PmacctDirectoryHandlerException(Exception):
    pass

class PmacctDirectoryHandler(RegexMatchingEventHandler):

    def __init__(self,
            name,
            local_as,
            default_as,
            renormalization_factor,
            graphite_prefix,
            graphite_server,
            graphite_port,
            debug,
            logger):
        self.name = name
        self.local_as = local_as
        self.default_as = default_as
        self.renormalization_factor = renormalization_factor
        self.graphite_prefix = graphite_prefix
        self.graphite_server = graphite_server
        self.graphite_port = graphite_port
        self.graphitesender = None
        self.debug = debug
        self.error_count = 0
        self._file_start_match_re = re.compile(r'^--START \(([0-9]+)\+([0-9]+)\)--$')
        self.logger = logger
        self.next_file = None
        RegexMatchingEventHandler.__init__( self,
                                            regexes=[r'^.*/pmacct_[0-9]+\.json$'],
                                            ignore_directories=True,
                                            case_sensitive=True
                                            )

    def _graphite_connect(self):
        try:
            self.graphitesender = graphitesend.init(
                    prefix=self.graphite_prefix,
                    system_name=self.name,
                    graphite_server=self.graphite_server,
                    graphite_port=self.graphite_port,
                    debug=self.debug)
        except Exception, e:
            raise PmacctDirectoryHandlerException('Graphite transport failed: \n%s' %(str(e)))

    def _renormalize(self, flow_bytes, flow_packets, renormalization_factor=None):
        if renormalization_factor is None:
            renormalization_factor=self.renormalization_factor
        if renormalization_factor==1:
            return (flow_bytes, flow_packets)
# usrf from pmacct:
#bpratio = queue[x]->bytes_counter/queue[x]->packet_counter;
#queue[x]->bytes_counter = queue[x]->bytes_counter*r;
#queue[x]->packet_counter = queue[x]->bytes_counter/bpratio; /* hmmm */
        bpratio = float(flow_bytes) / float(flow_packets)
        flow_bytes = flow_bytes * renormalization_factor
        flow_packets = flow_bytes / bpratio
        return (flow_bytes, flow_packets)


    def normalize_and_send_metric_list(self, metric_list, timestamp, interval):
        result_list = []
        for metric, flow_bytes, flow_packets in metric_list:
            # renormalize to fix sampling
            flow_bytes, flow_packets = self._renormalize(flow_bytes, flow_packets)
            # re-use renormalize function to calculate byte/packets per second
            flow_bytes, flow_packets = self._renormalize(flow_bytes, flow_packets, (1/float(interval)))
            result_list.append(('%s.bytes'   %(metric,), flow_bytes))
            result_list.append(('%s.packets' %(metric,), flow_packets))
        send_fail_count = 0
        while True:
            try:
                if not self.graphitesender:
                    self._graphite_connect()
                self.graphitesender.send_list(result_list, timestamp=(timestamp+interval/2))
            except Exception, e:
                self.graphitesender = None
                graphitesend.destroy()
                self.logger.critical('Graphite transport failed: \n%s\nTryin again!' %(str(e)))
                if send_fail_count > 25:
                    raise PmacctDirectoryHandlerException('Graphite transport failed: \n%s' %(str(e)))
                send_fail_count = send_fail_count + 1
                time.sleep(2)
            else:
                break


    def parse_pmacct_file(self, filename):
        self.logger.debug('Parsing %s' %(filename,))

        try:
            with open(filename) as f:
                content = f.readlines()
        except Exception, e:
            raise PmacctDirectoryHandlerException('Failed to read file %s: %s' %(filename, str(e)))

        pmacct_data=[ json.loads(x) for x in content[1:-1] ]
        if content[-1].strip() != '--END--':
            raise PmacctDirectoryHandlerException('--END-- tag missing in %s' %(filename,))
        try:
            timestamp, interval = self._file_start_match_re.match(content[0]).groups()
            timestamp = int(timestamp)
            interval = int(interval)
        except Exception, e:
            raise PmacctDirectoryHandlerException('--START... -- tag missing or not parsable in %s' %(filename,))

        # replace AS 0 by our default AS
        if self.default_as and self.default_as > 0:
            for entry in pmacct_data:
                for entry_key in ["as_dst", "as_src", "peer_dst", "peer_src"]:
                    if entry.has_key(entry_key) and entry[entry_key]==0:
                        entry[entry_key]=self.default_as

        return (pmacct_data, timestamp, interval)


    def on_created(self, event):
        self.logger.debug(event)
        super(PmacctDirectoryHandler, self).on_created(event)
        if event.is_directory:
            return
        self.handle_pmacct_file(event.src_path)


    def handle_pmacct_file(self, filename):
        # store new filename and wait for the next one to be created.
        current_file = self.next_file
        self.next_file = filename

        while current_file:
            try:
                pmacct_data, timestamp, interval = self.parse_pmacct_file(current_file)
                pmacct_metrics = self.parse_pmacct_data(pmacct_data, timestamp, interval)
                self._graphite_connect()
                self.normalize_and_send_metric_list(pmacct_metrics, timestamp, interval)
            except PmacctDirectoryHandlerException, e:
                self.logger.critical(e)
                self.error_count = self.error_count + 1
                time.sleep(10)
                if self.error_count >= 15:
                    raise
            else:
                self.error_count = 0
                os.remove(current_file)
                current_file = None


    def parse_pmacct_data(pmacct_data, timestamp, interval):
        """ parse pmacct_data into a list of metrics we can send.
            Needs to be overridden!"""
        pass


class PmacctASDirectoryHandler(PmacctDirectoryHandler):

    def parse_pmacct_data(self, pmacct_data, timestamp, interval):
        return self._parse_pmacct_data(pmacct_data, timestamp, interval, "as_dst", "as_src")

    def _parse_pmacct_data(self, pmacct_data, timestamp, interval, as_dst="as_dst", as_src="as_src"):
        metric_list = []
        for entry in pmacct_data:
            if entry[as_dst] in self.local_as or entry[as_dst] == self.default_as:
                metric_name = "%s.%s.in" %(str(entry[as_dst]), str(entry[as_src]))
            elif entry[as_src] in self.local_as or entry[as_src] == self.default_as:
                 metric_name = "%s.%s.out" %(str(entry[as_src]), str(entry[as_dst]))
            else:
                self.logger.critical("Neither %s nor %s known as local AS, can't handle %s!" %(str(entry[as_dst]),
                                                                                    str(entry[as_src]),
                                                                                    str(entry)))
                continue
            metric_list.append((metric_name, entry["bytes"], entry["packets"]))
        return metric_list
        
        
    
class PmacctPeerDirectoryHandler(PmacctASDirectoryHandler):

    def parse_pmacct_data(self, pmacct_data, timestamp, interval):
        return self._parse_pmacct_data(pmacct_data, timestamp, interval, "peer_dst", "peer_src")



class PmacctGraphiteD(object):


    def __init__(self,
                 local_as,
                 default_as,
                 pmacct_aggregations,
                 renormalization_factor,
                 graphite_prefix,
                 graphite_server,
                 graphite_port,
                 debug,
                 logger):
        self.stop_now = False
        self.observer = Observer()
        self.local_as = local_as
        self.default_as = default_as
        self.graphite_prefix = graphite_prefix
        self.pmacct_aggregations = pmacct_aggregations
        self.renormalization_factor = renormalization_factor
        self.graphite_server = graphite_server
        self.graphite_port = graphite_port
        self.debug = debug
        self.logger = logger

        self.eventhandler_types = {
            'as'    : PmacctASDirectoryHandler,
            'peer'  : PmacctPeerDirectoryHandler
        }


    def run(self):
        # setup observers
        for name, data in self.pmacct_aggregations.iteritems():
            handler = self.eventhandler_types[data['type']](
                    name,
                    self.local_as,
                    self.default_as,
                    self.renormalization_factor,
                    self.graphite_prefix,
                    self.graphite_server,
                    self.graphite_port,
                    self.debug,
                    self.logger)
            data['handler'] = handler
            self.observer.schedule(handler, data['spool_dirctory'], recursive=False)

        self.observer.start()


        # replay alrerady existing files
        pmacct_file_re = re.compile(r'^pmacct_[0-9]+\.json$')
        for name, data in self.pmacct_aggregations.iteritems():
            old_files = os.listdir(data['spool_dirctory'])
            old_files.sort()
            for filename in old_files:
                if pmacct_file_re.findall(filename):
                    data['handler'].handle_pmacct_file(os.path.join(data['spool_dirctory'], filename))

        while not self.stop_now:
            time.sleep(1)
        self.observer.stop()
        self.observer.join()

    def stop(self):
        self.stop_now = True

if __name__ == "__main__":

    from argparse import ArgumentParser
    from os.path import basename

    name = os.path.basename(sys.argv[0])

    epilog = '%s - delivering your pmacct statistics to graphite since 2014' %(name,)

    parser = ArgumentParser(epilog=epilog)
    parser.add_argument('-N', '--nodaemon',
                        action='store_true',
                        dest='nodaemon',
                        default=False,
                        help='do not run as daemon and log to stdout/stderr')
    parser.add_argument('-d', '--debug',
                        action='store_true',
                        dest='debug',
                        default=False,
                        help='enable debugging')
    parser.add_argument('-l', '--local-as',
                        action='append',
                        dest='local_as',
                        help='mark given AS number as local; can be specified as often as necessary',
                        type=int,
                        required=True)
    parser.add_argument('-a', '--default-as',
                        action='store',
                        dest='default_as',
                        help='default AS number, AS "0" will be replaced by this number in graphite',
                        default=None,
                        type=int)
    parser.add_argument('-g', '--graphite-prefix',
                        action='store',
                        dest='graphite_prefix',
                        help='prefix for generated graphite metric names',
                        default='network.pmacct',
                        type=str)
    parser.add_argument('-p', '--pmacct-aggregations',
                        action='append',
                        dest='pmacct_aggregations_args',
                        help='''pmacct aggregation name and directory we want to track;
                        Format: graphite_name:type:spool_directory;
                        Valid types: peer,as;
                        can be specified as often as necessary''',
                        required=True,
                        type=str)
    parser.add_argument('-r', '--renormalization-factor',
                        action='store',
                        dest='renormalization_factor',
                        help='''Applies the renormalization factor to counters,
                                suitable for use in conjunction with uniform sampling methods.''',
                        default=1,
                        type=int)
    parser.add_argument('-s', '--graphite-connection',
                        action='store',
                        dest='graphite_connection',
                        help='graphite server and port:  server:2013, default: 127.0.0.1:2003',
                        default='127.0.0.1:2003',
                        type=str)
    options = parser.parse_args()

    setproctitle('%s' %(name,) )

    loglevel = logging.INFO
    if options.debug:
        DEBUG=True
        loglevel = logging.DEBUG

    loghandler = logging.handlers.SysLogHandler()
    logging.getLogger('').addHandler(loghandler)
    logger = logging.getLogger('')
    logger.setLevel(loglevel)
      
    if not options.nodaemon:
        pidfile = process.runtime_file('%s.pid' % name)
        try:
            process.daemonize(pidfile)
        except ProcessError, e:
            logger.critical(str(e))
            sys.exit(1)

    logger.info("%s started" % name)


    # parse pmacct_aggregations into a dict
    pmacct_aggregations = {}
    for pmacct_aggregation in options.pmacct_aggregations_args:
        try:
            graphite_name, agg_type, spool_dirctory = pmacct_aggregation.split(':')
            if not graphite_name:
                raise Exception('graphite_name must not be empty!')
            graphite_name = graphite_name.strip('.')
            if not agg_type in ['as', 'peer']:
                raise Exception('unknown aggregation type "%s", please specify as or peer!' %(agg_type, ))
            if not os.path.isdir(spool_dirctory):
                raise Exception('directory "%s" does not exist' %(spool_dirctory))
            if pmacct_aggregations.has_key(graphite_name):
                raise Exception('please specify "%s" as aggregation name once only' %(graphite_name,))

        except Exception, e:
            logger.critical("Failed to parse option '-p %s': '%s'" %(pmacct_aggregation, str(e)))
            sys.exit(2)
        else:
            pmacct_aggregations[graphite_name] = {}
            pmacct_aggregations[graphite_name]['type'] = agg_type
            pmacct_aggregations[graphite_name]['spool_dirctory'] = spool_dirctory

    graphite_connection_re = re.compile(r'^\[?([^\]]*)]?:([0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])')
    try:
        graphite_server, graphite_port = graphite_connection_re.match(options.graphite_connection).groups()
        graphite_port=int(graphite_port)
    except Exception, e:
        logger.critical("Failed to parse graphite-connection option (%s given, needed format: server:port)" %(options.graphite_connection,))
        sys.exit(3)

    pgd = PmacctGraphiteD(options.local_as,
                          options.default_as,
                          pmacct_aggregations,
                          options.renormalization_factor,
                          options.graphite_prefix.strip('.'),
                          graphite_server,
                          graphite_port,
                          options.debug,
                          logger)

    try:
        pgd.run()
    except KeyboardInterrupt, SystemExit:
        pgd.stop()

